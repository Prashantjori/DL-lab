{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+4Xp6RAvKxEQcew7GZQPL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hitman-github/DL-lab/blob/main/Deeplprac5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBTFbciOnZDO",
        "outputId": "1c1565ca-c9ec-462c-cdc9-9a5effb5ccd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for \"continuous\": [-0.03124946  0.03465173  0.00292311  0.04339533  0.03308858  0.00866221\n",
            "  0.03405542 -0.0151189  -0.00933057 -0.04688987  0.02582873 -0.0194653\n",
            " -0.03136665 -0.04036502 -0.04578265 -0.00329898  0.01768721  0.03952796\n",
            "  0.04584498 -0.03387552  0.02450829 -0.02136362  0.02331029  0.04992907\n",
            " -0.0251595   0.02584947  0.01368916 -0.04238601 -0.0368206   0.01318756\n",
            " -0.0298368   0.01067501  0.03886915 -0.00677271  0.04957129 -0.0306794\n",
            " -0.01404453 -0.04390711 -0.00633026 -0.03194531 -0.00915004 -0.03695065\n",
            " -0.03090031 -0.04426117 -0.0189038  -0.01314871  0.03649947  0.04550507\n",
            " -0.02096957 -0.04051362 -0.02050894 -0.03205862 -0.01343675  0.03988351\n",
            " -0.02660288  0.03022819 -0.03860588 -0.04777876  0.04356274  0.02005536\n",
            " -0.02966841 -0.03325192  0.00634824 -0.04137701  0.00370952 -0.032657\n",
            "  0.04026326 -0.01808903  0.0203308   0.04779037 -0.03426311  0.0077642\n",
            " -0.00976863 -0.04128233 -0.00284312 -0.03468702 -0.03175881 -0.03462393\n",
            " -0.00669756 -0.04314953  0.0074312  -0.02636808 -0.02667917  0.00793558\n",
            " -0.03993647 -0.02404208 -0.04364878 -0.01159374 -0.0460006   0.03912817\n",
            "  0.03372637 -0.01303722  0.03982291 -0.02551167  0.04939711  0.04332895\n",
            " -0.00674618 -0.02824069  0.02694973  0.0044504 ]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "# Example text data\n",
        "text = \"continuous bag of words model is a simple and effective method for word embeddings\"\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "# Create word-to-index and index-to-word mapping\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "# Generate training data\n",
        "vocab_size = len(word2idx) + 1 # Add 1 for padding token\n",
        "context_window = 2 # Adjust context window size\n",
        "X, y = [], []\n",
        "for i in range(context_window, len(sequences) - context_window):\n",
        "  context = sequences[i - context_window:i] + sequences[i + 1:i + context_window + 1]\n",
        "target = sequences[i]\n",
        "X.append(context)\n",
        "y.append(target)\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Pad the sequences to the same length\n",
        "max_sequence_length = context_window * 2\n",
        "X = pad_sequences(X, maxlen=max_sequence_length)\n",
        "# Define the CBOW model\n",
        "model = keras.Sequential([\n",
        "keras.layers.Embedding(input_dim=vocab_size, output_dim=100, input_length=max_sequence_length),\n",
        "keras.layers.GlobalAveragePooling1D(),\n",
        "keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "# Extract the word embeddings\n",
        "word_embeddings = model.layers[0].get_weights()[0]\n",
        "# Get the word embedding for a specific word\n",
        "word_to_lookup = 'continuous'\n",
        "word_idx = word2idx[word_to_lookup]\n",
        "embedding = word_embeddings[word_idx]\n",
        "print(f'Embedding for \"{word_to_lookup}\": {embedding}')"
      ]
    }
  ]
}